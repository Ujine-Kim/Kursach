{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c01572d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.7.1\n",
      "numpy: 1.20.3\n",
      "matplotlib: 3.4.3\n",
      "pandas: 1.3.4\n",
      "statsmodels: 0.12.2\n",
      "sklearn: 0.24.2\n",
      "tensorflow: 2.8.0\n",
      "keras: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "print('scipy: %s' % scipy.__version__)\n",
    "# numpy\n",
    "import numpy as np\n",
    "print('numpy: %s' % np.__version__)\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: %s' % matplotlib.__version__)\n",
    "# pandas\n",
    "import pandas\n",
    "print('pandas: %s' % pandas.__version__)\n",
    "# statsmodels\n",
    "import statsmodels\n",
    "print('statsmodels: %s' % statsmodels.__version__)\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__)\n",
    "# tensorflow\n",
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "#keras\n",
    "import keras\n",
    "print('keras: %s' % keras.__version__)\n",
    "from pickle import load\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,TimeDistributed,Activation, Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from tensorflow.keras.layers import Input, RepeatVector\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4490ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Загрузка данных\n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Разбивание в массив\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    pairs = np.delete(pairs,2,1)\n",
    "    return pairs\n",
    "\n",
    "# Очистка данных от знаков препинания + перевод в нижний регистр\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            line = line.split()\n",
    "            line = [word.lower() for word in line]\n",
    "            line = [word.translate(table) for word in line]\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    "\n",
    "# Сохранение в pkl\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "filename = 'rus.txt'\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "print(pairs)\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "print(clean_pairs)\n",
    "save_clean_data(clean_pairs, 'english-russian.pkl')\n",
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ff51f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-russian-both.pkl\n",
      "Saved: english-russian-train.pkl\n",
      "Saved: english-russian-test.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Загрузка уже чистых данных\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# Сохрание чистых данных\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# загрузка набора данных\n",
    "raw_dataset = load_clean_sentences('english-russian.pkl')\n",
    "\n",
    "# Уменьшение кол-ва примеров\n",
    "n_sentences = 50000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# Перемешка\n",
    "shuffle(dataset)\n",
    "# Разделения на обучающие и проверчноые примеры\n",
    "train, test = dataset[:45000], dataset[45000:]\n",
    "# Сохранение\n",
    "save_clean_data(dataset, 'english-russian-both.pkl')\n",
    "save_clean_data(train, 'english-russian-train.pkl')\n",
    "save_clean_data(test, 'english-russian-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b885122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 5065\n",
      "English Max Length: 6\n",
      "Russian Vocabulary Size: 13792\n",
      "Russian Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "# Токенизация Английского словаря\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# Токенизация Русского словаря\n",
    "rus_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
    "rus_length = max_length(dataset[:, 1])\n",
    "print('Russian Vocabulary Size: %d' % rus_vocab_size)\n",
    "print('Russian Max Length: %d' % (rus_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d1b5dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  33   25 4059 ...    0    0    0]\n",
      " [  48  985    0 ...    0    0    0]\n",
      " [   1  763  101 ...    0    0    0]\n",
      " ...\n",
      " [ 406    4   19 ...    0    0    0]\n",
      " [   2 4502 4835 ...    0    0    0]\n",
      " [1970  276    6 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "trainX = encode_sequences(rus_tokenizer, rus_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "testX = encode_sequences(rus_tokenizer, rus_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "975245a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 10, 256)           3530752   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector_2 (RepeatVect  (None, 6, 256)           0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 6, 256)            525312    \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 6, 5065)          1301705   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,883,081\n",
      "Trainable params: 5,883,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 3.03395, saving model to model.h5\n",
      "704/704 - 89s - loss: 3.4969 - val_loss: 3.0339 - 89s/epoch - 126ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 3.03395 to 2.53674, saving model to model.h5\n",
      "704/704 - 85s - loss: 2.7282 - val_loss: 2.5367 - 85s/epoch - 120ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 2.53674 to 2.19368, saving model to model.h5\n",
      "704/704 - 85s - loss: 2.2607 - val_loss: 2.1937 - 85s/epoch - 120ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 2.19368 to 1.93525, saving model to model.h5\n",
      "704/704 - 84s - loss: 1.8941 - val_loss: 1.9353 - 84s/epoch - 120ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss improved from 1.93525 to 1.75013, saving model to model.h5\n",
      "704/704 - 85s - loss: 1.5953 - val_loss: 1.7501 - 85s/epoch - 120ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 1.75013 to 1.60388, saving model to model.h5\n",
      "704/704 - 86s - loss: 1.3451 - val_loss: 1.6039 - 86s/epoch - 122ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 1.60388 to 1.48540, saving model to model.h5\n",
      "704/704 - 86s - loss: 1.1277 - val_loss: 1.4854 - 86s/epoch - 123ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss improved from 1.48540 to 1.38748, saving model to model.h5\n",
      "704/704 - 85s - loss: 0.9420 - val_loss: 1.3875 - 85s/epoch - 120ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 1.38748 to 1.31506, saving model to model.h5\n",
      "704/704 - 85s - loss: 0.7879 - val_loss: 1.3151 - 85s/epoch - 121ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 1.31506 to 1.26754, saving model to model.h5\n",
      "704/704 - 84s - loss: 0.6614 - val_loss: 1.2675 - 84s/epoch - 119ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 1.26754 to 1.24377, saving model to model.h5\n",
      "704/704 - 84s - loss: 0.5608 - val_loss: 1.2438 - 84s/epoch - 119ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss improved from 1.24377 to 1.21587, saving model to model.h5\n",
      "704/704 - 83s - loss: 0.4809 - val_loss: 1.2159 - 83s/epoch - 118ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 1.21587 to 1.20383, saving model to model.h5\n",
      "704/704 - 83s - loss: 0.4165 - val_loss: 1.2038 - 83s/epoch - 117ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.20383\n",
      "704/704 - 83s - loss: 0.3651 - val_loss: 1.2066 - 83s/epoch - 117ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 1.20383 to 1.20026, saving model to model.h5\n",
      "704/704 - 83s - loss: 0.3254 - val_loss: 1.2003 - 83s/epoch - 118ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 1.20026 to 1.19964, saving model to model.h5\n",
      "704/704 - 83s - loss: 0.2929 - val_loss: 1.1996 - 83s/epoch - 118ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.19964\n",
      "704/704 - 83s - loss: 0.2657 - val_loss: 1.2028 - 83s/epoch - 117ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.19964\n",
      "704/704 - 102s - loss: 0.2438 - val_loss: 1.2091 - 102s/epoch - 145ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.19964\n",
      "704/704 - 84s - loss: 0.2259 - val_loss: 1.2142 - 84s/epoch - 119ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.19964\n",
      "704/704 - 83s - loss: 0.2120 - val_loss: 1.2242 - 83s/epoch - 119ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.19964\n",
      "704/704 - 85s - loss: 0.2002 - val_loss: 1.2352 - 85s/epoch - 120ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.19964\n",
      "704/704 - 84s - loss: 0.1888 - val_loss: 1.2491 - 84s/epoch - 120ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.19964\n",
      "704/704 - 84s - loss: 0.1828 - val_loss: 1.2628 - 84s/epoch - 120ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.19964\n",
      "704/704 - 84s - loss: 0.1741 - val_loss: 1.2649 - 84s/epoch - 120ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.19964\n",
      "704/704 - 84s - loss: 0.1671 - val_loss: 1.2705 - 84s/epoch - 119ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.19964\n",
      "704/704 - 84s - loss: 0.1627 - val_loss: 1.2702 - 84s/epoch - 119ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.19964\n",
      "704/704 - 83s - loss: 0.1574 - val_loss: 1.2823 - 83s/epoch - 118ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.19964\n",
      "704/704 - 83s - loss: 0.1537 - val_loss: 1.3037 - 83s/epoch - 118ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.19964\n",
      "704/704 - 85s - loss: 0.1527 - val_loss: 1.2977 - 85s/epoch - 121ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.19964\n",
      "704/704 - 84s - loss: 0.1481 - val_loss: 1.3121 - 84s/epoch - 119ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17baf6aa190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "model = define_model(rus_vocab_size, eng_vocab_size, rus_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ae7e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[кто вас избил], target=[who beat you up], predicted=[who nails you]\n",
      "src=[сейчас полтретьего], target=[its now 230], predicted=[now that]\n",
      "src=[я боюсь идти], target=[i am afraid to go], predicted=[im going to]\n",
      "src=[он огляделся], target=[he looked around], predicted=[hes is]\n",
      "src=[будьте кратки пожалуйста], target=[be brief please], predicted=[please be please]\n",
      "src=[ты была несчастна], target=[were you unhappy], predicted=[you you eat]\n",
      "src=[это не моя машина], target=[thats not my car], predicted=[its isnt my car]\n",
      "src=[ты не устала], target=[werent you tired], predicted=[you you go in]\n",
      "src=[разбей это], target=[smash it], predicted=[modest it]\n",
      "src=[я не ем], target=[i am not eating], predicted=[i not a put]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koiko\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\koiko\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.079154\n",
      "BLEU-2: 0.000903\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n",
      "test\n",
      "src=[мы его исправим], target=[well fix it], predicted=[we okay it]\n",
      "src=[вы мне надоели], target=[im sick of you], predicted=[did you me me]\n",
      "src=[не прикасайся ко мне], target=[dont touch me], predicted=[keep on of me]\n",
      "src=[я не так уж и устал], target=[im not so tired], predicted=[im not still day]\n",
      "src=[том увидел кошку], target=[tom saw the cat], predicted=[tom will theirs]\n",
      "src=[вставай с постели], target=[get up out of bed], predicted=[get up next]\n",
      "src=[я видел нескольких], target=[ive seen a few], predicted=[i saw a sacred]\n",
      "src=[можно потрогать], target=[can i touch it], predicted=[may can an fishing]\n",
      "src=[ловите мяч], target=[catch the ball], predicted=[everyone it gate]\n",
      "src=[у тебя дети есть], target=[do you have kids], predicted=[do you have a]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koiko\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.073889\n",
      "BLEU-2: 0.000000\n",
      "BLEU-3: 0.000000\n",
      "BLEU-4: 0.000000\n"
     ]
    }
   ],
   "source": [
    "def word_for_id(interus, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == interus:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    interuss = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in interuss:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "dataset = load_clean_sentences('english-russian-both.pkl')\n",
    "train = load_clean_sentences('english-russian-train.pkl')\n",
    "test = load_clean_sentences('english-russian-test.pkl')\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "rus_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
    "rus_length = max_length(dataset[:, 1])\n",
    "trainX = encode_sequences(rus_tokenizer, rus_length, train[:, 1])\n",
    "testX = encode_sequences(rus_tokenizer, rus_length, test[:, 1])\n",
    "model = load_model('model.h5')\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0849db",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tokenizer' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21436/1142027411.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meng_tokenizer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Tokenizer' object is not subscriptable"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
